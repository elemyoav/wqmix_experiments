[INFO 03:31:31] pymarl Running command 'my_main'
[INFO 03:31:31] pymarl Started run with ID "1"
[DEBUG 03:31:31] my_main Started
[INFO 03:31:31] my_main Experiment Parameters:
[INFO 03:31:31] my_main 

{   'accumulated_episodes': 8,
    'action_selector': 'multinomial',
    'agent': 'n_rnn',
    'agent_output_type': 'pi_logits',
    'batch_size': 64,
    'batch_size_run': 8,
    'buffer_cpu_only': True,
    'buffer_size': 64,
    'checkpoint_path': '',
    'critic_coef': 0.5,
    'critic_lr': 0.0005,
    'entropy': 0.01,
    'env': 'team_rock_sampling',
    'env_args': {   'env_args': {   'horizon': 100},
                    'map_name': 'team_rock_sampling',
                    'seed': 458414770},
    'eps_clip': 0.2,
    'epsilon_anneal_time': 100000,
    'epsilon_finish': 0.0,
    'epsilon_start': 0.0,
    'evaluate': False,
    'gae_lambda': 0.95,
    'gain': 0.01,
    'gamma': 0.99,
    'grad_norm_clip': 10,
    'label': 'default_label',
    'learner': 'ppo_learner',
    'learner_log_interval': 2000,
    'load_step': 0,
    'local_results_path': 'results',
    'log_interval': 2000,
    'lr': 0.0005,
    'mac': 'basic_mac',
    'mask_before_softmax': True,
    'mini_epochs': 8,
    'name': 'ippo_env=8_adam_gae',
    'obs_agent_id': True,
    'obs_last_action': True,
    'optim_alpha': 0.99,
    'optim_eps': 1e-05,
    'repeat_id': 1,
    'rnn_hidden_dim': 64,
    'run': 'default',
    'runner': 'parallel',
    'runner_log_interval': 2000,
    'save_model': False,
    'save_model_interval': 2000000,
    'save_probs': True,
    'save_replay': False,
    'seed': 458414770,
    't_max': 10050000,
    'test_greedy': True,
    'test_interval': 20000,
    'test_nepisode': 24,
    'use_cuda': True,
    'use_layer_norm': True,
    'use_orthogonal': True,
    'use_tensorboard': False,
    'use_value_norm': True}

[INFO 03:31:34] my_main Beginning training for 10050000 timesteps
/home/elem/repos/MARL/pymarl/src/components/episode_buffer.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  v = th.tensor(v, dtype=dtype, device=self.device)
[INFO 03:31:34] my_main t_env: 800 / 10050000
[INFO 03:31:34] my_main Estimated time left: 3 minutes, 24 seconds. Time passed: 0 seconds
[INFO 03:31:36] my_main Recent Stats | t_env:       2400 | Episode:       24
ep_length_mean:          100.0000	epsilon:                   0.0000	return_mean:             -357.6250	return_std:              266.9293
test_ep_length_mean:     100.0000	test_return_mean:        -356.8333	test_return_std:         232.2484	
[INFO 03:31:37] my_main Recent Stats | t_env:       4800 | Episode:       48
ep_length_mean:          100.0000	epsilon:                   0.0000	return_mean:             -363.6667	return_std:              302.4473

[INFO 03:31:42] my_main Recent Stats | t_env:       7200 | Episode:       72
actor_loss:               -0.0263	advantage_mean:            0.0000	critic_loss:               0.4897	entropy_loss:              6.2621
ep_length_mean:          100.0000	epsilon:                   0.0000	grad_norm:                 0.1825	lr:                        0.0005
return_mean:             -428.0417	return_std:               39.9453	target_mean:              -0.0000	
[INFO 03:31:49] my_main Recent Stats | t_env:       9600 | Episode:       96
actor_loss:               -0.0350	advantage_mean:           -0.0000	critic_loss:               0.2747	entropy_loss:              6.2491
ep_length_mean:          100.0000	epsilon:                   0.0000	grad_norm:                 0.1064	lr:                        0.0005
return_mean:             -282.6667	return_std:              367.9963	target_mean:              -0.6347	
[INFO 03:31:56] my_main Recent Stats | t_env:      12000 | Episode:      120
actor_loss:               -0.0419	advantage_mean:            0.0000	critic_loss:               0.2668	entropy_loss:              6.2264
ep_length_mean:          100.0000	epsilon:                   0.0000	grad_norm:                 0.3375	lr:                        0.0005
return_mean:             -364.0833	return_std:              143.9285	target_mean:              -0.8337	
[INFO 03:32:06] my_main Recent Stats | t_env:      14400 | Episode:      144
actor_loss:               -0.0432	advantage_mean:           -0.0000	critic_loss:               0.3576	entropy_loss:              6.2083
ep_length_mean:          100.0000	epsilon:                   0.0000	grad_norm:                 0.5301	lr:                        0.0005
return_mean:             -338.6250	return_std:              160.4369	target_mean:              -0.7579	
